{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "## Action: Understanding Gradient Descent Algorithm\n",
    "\n",
    "In this section, we will first run the gradient descent algorithm and then explain why it works the way it does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Main Goal: Understand gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What it's gradient descent?\n",
    "\n",
    "It's an algorithm that helps us eventually find the optimal point. Visually, what it does is take small steps in the direction of the steepest descent for a particular objective function. In machine learning, objective functions are more commonly known as loss or cost functions.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://www.researchgate.net/publication/372985352/figure/fig2/AS:11431281180215918@1691514092819/Gradient-descent-works-in-the-case-of-one-dimensional-variables-7.jpg\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "If we focus on the mathematical function, we can define it in two cases:\n",
    "\n",
    "**1. Gradient for a function of two variables:**\n",
    "$$\n",
    "\\nabla f(x, y) = \\frac{\\partial f}{\\partial x} \\mathbf{i} + \\frac{\\partial f}{\\partial y} \\mathbf{j}\n",
    "$$\n",
    "\n",
    "**2. Gradient for a function of three variables:**\n",
    "$$\n",
    "\\nabla f(x, y, z) = \\frac{\\partial f}{\\partial x} \\mathbf{i} + \\frac{\\partial f}{\\partial y} \\mathbf{j} + \\frac{\\partial f}{\\partial z} \\mathbf{k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Let's Code this Up! ðŸ’»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sympy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mWe're going to use sympy python library to solve the equations\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sympy'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We're going to use sympy python library to solve the equations\n",
    "\"\"\"\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the variables\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Step 2: Define the function\n",
    "f = x**2 + y**2\n",
    "\n",
    "# Step 3: Compute the partial derivatives\n",
    "partial_x = sp.diff(f, x)\n",
    "partial_y = sp.diff(f, y)\n",
    "\n",
    "# Step 4: Let's get our gradient as a vector\n",
    "gradient_2d = (partial_x, partial_y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
