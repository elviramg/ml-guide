{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "## Action: Understanding Gradient Descent Algorithm\n",
    "\n",
    "In this section, we will first run the gradient descent algorithm and then explain why it works the way it does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Main Goal: Understand gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What it's gradient descent?\n",
    "\n",
    "It's an algorithm that helps us eventually find the optimal point. Visually, what it does is take small steps in the direction of the steepest descent for a particular objective function. In machine learning, objective functions are more commonly known as loss or cost functions.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"/Users/elviramagallanes/code/elviramg/ml-guide/photos/Gradient-descent-works-in-the-case-of-one-dimensional-variables-7.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "If we focus on the mathematical function, we can define it in two cases:\n",
    "\n",
    "**1. Gradient for a function of two variables:**\n",
    "$$\n",
    "\\nabla f(x, y) = \\frac{\\partial f}{\\partial x} \\mathbf{i} + \\frac{\\partial f}{\\partial y} \\mathbf{j}\n",
    "$$\n",
    "\n",
    "**2. Gradient for a function of three variables:**\n",
    "$$\n",
    "\\nabla f(x, y, z) = \\frac{\\partial f}{\\partial x} \\mathbf{i} + \\frac{\\partial f}{\\partial y} \\mathbf{j} + \\frac{\\partial f}{\\partial z} \\mathbf{k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's Code this Up! ðŸ’»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We're going to use sympy python library to solve the equations\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic gradient in vector notation:\n",
      "(- 3 x^{2} y^{2})i + (- 2 x^{3} y + 5)j\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the variables\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Step 2: Define the function. Example function: f(x,y) = 5y - xÂ³yÂ²\n",
    "f = 5*y - x**3 * y**2\n",
    "\n",
    "# Step 3: Compute the partial derivatives\n",
    "partial_x = sp.diff(f, x)\n",
    "partial_y = sp.diff(f, y)\n",
    "\n",
    "# Step 4: Let's get our gradient as a vector\n",
    "gradient_2d = (partial_x, partial_y)\n",
    "print(\"Symbolic gradient in vector notation:\")\n",
    "gradient_vector = f\"({sp.latex(partial_x)})i + ({sp.latex(partial_y)})j\"\n",
    "print(gradient_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we've implemented the mathematical concept! But how can we unify this with the algorithm everyone talks about in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's generate some random data to work with\n",
    "\"\"\"\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 + 3 * X + np.random.randn(100, 1) * 0.1\n",
    "\n",
    "# Initialize parameters\n",
    "b0 = 0  # y-intercept\n",
    "b1 = 0  # slope\n",
    "eta = 0.1  # learning rate\n",
    "epochs = 1000  # number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is our hypothesis function, representing a straight line where b0 is the y-intercept \n",
    "and b1 is the slope.\n",
    "\"\"\"\n",
    "def h(X, b0, b1):\n",
    "    return b0 + b1 * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's define our cost function\n",
    "\"\"\"\n",
    "def cost_function(X, y, b0, b1):\n",
    "    return np.mean((y - h(X, b0, b1)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's implement the gradient descent algorithm! \n",
    "def gradient_descent(X, y, b0, b1, eta, epochs):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        # Compute predictions\n",
    "        y_pred = h(X, b0, b1)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_b0 = (-2/m) * np.sum(y - y_pred)\n",
    "        grad_b1 = (-2/m) * np.sum((y - y_pred) * X)\n",
    "        \n",
    "        # Update parameters\n",
    "        b0 = b0 - eta * grad_b0\n",
    "        b1 = b1 - eta * grad_b1\n",
    "        \n",
    "        # Compute and store the cost\n",
    "        cost = cost_function(X, y, b0, b1)\n",
    "        cost_history.append(cost)\n",
    "    \n",
    "    return b0, b1, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_final, b1_final, cost_history = gradient_descent(X, y, b0, b1, eta, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final b0: 2.0222\n",
      "Final b1: 2.9937\n",
      "Final cost: 0.0099\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final b0: {b0_final:.4f}\")\n",
    "print(f\"Final b1: {b1_final:.4f}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
