{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#800020;\">Gradient Descent</h1>\n",
    "\n",
    "## Action: \n",
    "### Understanding Gradient Descent Algorithm\n",
    "\n",
    "In this section, we will first run the gradient descent algorithm and then explain why it works the way it does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Main Goal: Understand gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What it's gradient descent?\n",
    "\n",
    "It's an algorithm that helps us eventually find the optimal point. Visually, what it does is take small steps in the direction of the steepest descent for a particular objective function. In machine learning, objective functions are more commonly known as loss or cost functions.\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"../photos/gradient-descent.png\" alt=\"Gradient Descent Visualization\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "If we focus on the mathematical function, we can define it in two cases:\n",
    "\n",
    "**1. Gradient for a function of two variables:**\n",
    "$$\n",
    "\\nabla f(x, y) = \\frac{\\partial f}{\\partial x} \\mathbf{i} + \\frac{\\partial f}{\\partial y} \\mathbf{j}\n",
    "$$\n",
    "\n",
    "**2. Gradient for a function of three variables:**\n",
    "$$\n",
    "\\nabla f(x, y, z) = \\frac{\\partial f}{\\partial x} \\mathbf{i} + \\frac{\\partial f}{\\partial y} \\mathbf{j} + \\frac{\\partial f}{\\partial z} \\mathbf{k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's Code this Up! ðŸ’»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We're going to use sympy python library to solve the equations\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic gradient in vector notation:\n",
      "(- 3 x^{2} y^{2})i + (- 2 x^{3} y + 5)j\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the variables\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Step 2: Define the function. Example function: f(x,y) = 5y - xÂ³yÂ²\n",
    "f = 5*y - x**3 * y**2\n",
    "\n",
    "# Step 3: Compute the partial derivatives\n",
    "partial_x = sp.diff(f, x)\n",
    "partial_y = sp.diff(f, y)\n",
    "\n",
    "# Step 4: Let's get our gradient as a vector\n",
    "gradient_2d = (partial_x, partial_y)\n",
    "print(\"Symbolic gradient in vector notation:\")\n",
    "gradient_vector = f\"({sp.latex(partial_x)})i + ({sp.latex(partial_y)})j\"\n",
    "print(gradient_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we've implemented the mathematical concept! But how can we unify this with the algorithm everyone talks about in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's generate some random data to work with\n",
    "\"\"\"\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 + 3 * X + np.random.randn(100, 1) * 0.1\n",
    "\n",
    "# Initialize parameters\n",
    "b0 = 0  # y-intercept\n",
    "b1 = 0  # slope\n",
    "learning_rate = 0.1  # learning rate\n",
    "epochs = 1000  # number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is our hypothesis function, representing a straight line where b0 is the y-intercept \n",
    "and b1 is the slope.\n",
    "\"\"\"\n",
    "def h(X, b0, b1):\n",
    "    return b0 + b1 * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's define our cost function\n",
    "\"\"\"\n",
    "def cost_function(X, y, b0, b1):\n",
    "    return np.mean((y - h(X, b0, b1)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's implement the gradient descent algorithm! \n",
    "def gradient_descent(X, y, b0, b1, learning_rate, epochs):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        # Compute predictions\n",
    "        y_pred = h(X, b0, b1)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_b0 = (-2/m) * np.sum(y - y_pred)\n",
    "        grad_b1 = (-2/m) * np.sum((y - y_pred) * X)\n",
    "        \n",
    "        # Update parameters\n",
    "        b0 = b0 - learning_rate * grad_b0\n",
    "        b1 = b1 - learning_rate * grad_b1\n",
    "        \n",
    "        # Compute and store the cost\n",
    "        cost = cost_function(X, y, b0, b1)\n",
    "        cost_history.append(cost)\n",
    "    \n",
    "    return b0, b1, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_final, b1_final, cost_history = gradient_descent(X, y, b0, b1, eta, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final b0: 2.0222\n",
      "Final b1: 2.9937\n",
      "Final cost: 0.0099\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final b0: {b0_final:.4f}\")\n",
    "print(f\"Final b1: {b1_final:.4f}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfect! So let's recap what we did:**\n",
    "\n",
    "1. Starts with initial guesses for the line's slope and intercept.\n",
    "2. Repeats the following steps for a set number of times.\n",
    "3. Makes predictions using the current slope and intercept.\n",
    "4. Calculates how wrong these predictions are.\n",
    "5. Adjusts the slope and intercept to make the predictions a little better.\n",
    "5. Keeps track of how well it's doing.\n",
    "\n",
    "Finally, it returns the best slope and intercept it found, along with a record of its progress. ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "### What's the difference?\n",
    "\n",
    "The key difference from what we looked up before is that in batch, we take into account ALL training data to compute the gradient and update the model parameters after every pass (epoch) through the entire dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-guide-env",
   "language": "python",
   "name": "ml-guide-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
